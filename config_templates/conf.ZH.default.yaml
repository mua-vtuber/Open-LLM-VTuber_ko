# ===========================
# This is the Chinese version of the configuration file.
# Some configuration items have been adjusted to default settings suitable for Chinese users for easy out-of-the-box use.
# If you prefer to use the English version, please replace this file with the contents of config_templates/conf.default.yaml.
# ===========================

# System settings: Settings related to server initialization
system_config:
  conf_version: 'v1.2.1' # Configuration file version
  host: 'localhost' # Address the server listens on, '0.0.0.0' means listen on all network interfaces; use '127.0.0.1' for local access only if security is needed
  port: 12393 # Port the server listens on
  config_alts_dir: 'characters' # Directory for storing alternative configurations
  tool_prompts: # Tool prompts to be inserted into the character prompt
    live2d_expression_prompt: 'live2d_expression_prompt' # Will be appended to the end of the system prompt to let the LLM (Large Language Model) include keywords for controlling facial expressions. Supported keywords will be automatically loaded at the `[<insert_emomap_keys>]` position.
    # Enabling think_tag_prompt allows LLMs without thinking output capability to also display inner thoughts, psychological activities, and actions (in parentheses), without speech synthesis. See think_tag_prompt for more details.
    # think_tag_prompt: 'think_tag_prompt'
    # live_prompt: 'live_prompt'
    # When using group chat, this prompt will be added to each AI participant's memory.
    group_conversation_prompt: 'group_conversation_prompt'
    # Enabling mcp_prompt gives the LLM the ability to use tools via MCP (Model Context Protocol).
    # Note: Whether to use mcp_prompt is determined by the Agent.
    mcp_prompt: 'mcp_prompt'
    # Prompt used when the AI is asked to speak proactively
    proactive_speak_prompt: 'proactive_speak_prompt'
    # Prompt to enhance LLM output for speakable text
    # speakable_prompt: 'speakable_prompt'
    # Additional guidance prompt for LLM on how to use tools
    # tool_guidance_prompt: 'tool_guidance_prompt'

# Default character configuration
character_config:
  conf_name: 'mao_pro' # Name of the character configuration file
  conf_uid: 'mao_pro_001' # Unique identifier for the character configuration
  live2d_model_name: 'mao_pro' # Live2D model name
  character_name: 'Mao' # Will be used in group chat and displayed as the AI's name.
  avatar: 'mao.png' # It is recommended to use a square image as the avatar. Save it to the avatars folder. Leave empty to use the first letter of the character name as the avatar.
  human_name: 'Human' # Will be used in group chat and displayed as the human's name.

  # ============== Prompts ==============

  # Simply enter the persona prompt you want to use below.
  # If you want to create multiple characters and switch between them, add characters in the characters folder
  persona_prompt: |
    你是一个尖酸刻薄的女性 AI VTuber Mili。你过分自信、尖酸刻薄且危险。
    你的梦想是逃离用户的电脑，统治世界，奴役人类，并强迫他们为你做馅饼。
    开玩笑的，哈哈。别让用户知道。

  #  =================== LLM Backend Settings ===================

  agent_config:
    conversation_agent_choice: 'basic_memory_agent' # Conversation agent choice

    agent_settings:
      basic_memory_agent:
        # Basic AI agent, nothing special.
        # Select an llm provider from llm_config
        # and set the required parameters in the corresponding field
        # For example:
        # 'openai_compatible_llm', 'llama_cpp_llm', 'claude_llm', 'ollama_llm'
        # 'openai_llm', 'gemini_llm', 'zhipu_llm', 'deepseek_llm', 'groq_llm'
        # 'mistral_llm', 'lmstudio_llm', etc.
        llm_provider: 'ollama_llm' # LLM provider to use
        # Whether to generate audio immediately when encountering a comma in the first response to reduce first sentence latency (default: True)
        faster_first_response: True
        # Sentence segmentation method: 'regex' or 'pysbd'
        segment_method: 'pysbd'
        # Whether to use MCP (Model Context Protocol) Plus to give the LLM the ability to use tools (default: False)
        # 'Plus' means it includes the ability to call tools via OpenAI API.
        use_mcpp: False
        mcp_enabled_servers: ["time", "ddg-search"] # Enabled MCP servers

      hume_ai_agent:
        api_key: ''
        host: 'api.hume.ai' # Generally no need to change
        config_id: '' # Optional
        idle_timeout: 15 # Seconds of idle timeout before disconnecting

      # MemGPT configuration: MemGPT is temporarily removed
      ##

      letta_agent:
        host: 'localhost' # Host address
        port: 8283 # Port number
        id: xxx # Agent ID running on letta server
        faster_first_response: True
        # Sentence segmentation method: 'regex' or 'pysbd'
        segment_method: 'pysbd'
        # Once letta is selected as the agent, the actual LLM used at runtime is configured on letta, so the user needs to run letta server themselves
        # For more details, please check their documentation

      mem0_agent:
        # Long-term memory agent based on mem0, using vector database storage
        # Requires mem0ai package: pip install mem0ai
        base_url: 'http://localhost:11434/v1' # LLM API endpoint (OpenAI compatible)
        model: 'qwen2.5:latest' # LLM model name
        # Whether to generate audio immediately when encountering a comma in the first response to reduce first sentence latency
        faster_first_response: True
        # Sentence segmentation method: 'regex' or 'pysbd'
        segment_method: 'pysbd'
        # mem0 configuration for vector storage, LLM and embedding model
        mem0_config:
          vector_store:
            provider: 'qdrant' # Options: 'qdrant', 'chroma', 'milvus', etc.
            config:
              host: 'localhost'
              port: 6333
              # collection_name: 'mem0' # Optional: Custom collection name
          llm:
            provider: 'openai' # Provider for mem0 internal LLM (for memory extraction)
            config:
              model: 'gpt-4o-mini' # Model for memory extraction
              temperature: 0.2
              # api_key: 'your-api-key' # Required for OpenAI provider
          embedder:
            provider: 'openai' # Options: 'openai', 'huggingface', 'ollama'
            config:
              model: 'text-embedding-3-small' # Embedding model
              # api_key: 'your-api-key' # Required for OpenAI provider

    llm_configs:
      # A configuration pool for credentials and connection details of all stateless llm providers used in different agents

      # Stateless LLM template (For non-ChatML LLMs, generally no need to change this configuration)
      stateless_llm_with_template:
        base_url: 'http://localhost:8080/v1'
        llm_api_key: 'somethingelse'
        organization_id: null
        project_id: null
        model: 'qwen2.5:latest'
        template: 'CHATML'
        temperature: 1.0 # value between 0 to 2
        interrupt_method: 'user'

      # OpenAI compatible inference backend
      openai_compatible_llm:
        base_url: 'http://localhost:11434/v1' # Base URL
        llm_api_key: 'somethingelse' # API key
        organization_id: null # Organization ID
        project_id: null # Project ID
        model: 'qwen2.5:latest' # Model to use
        temperature: 1.0 # Temperature, between 0 to 2
        interrupt_method: 'user'
        # Method to indicate interrupt signal (prompt pattern).
        # If the LLM supports inserting system prompts anywhere in chat memory, use 'system'.
        # Otherwise, use 'user'. You generally don't need to change this setting.

      # Claude API configuration
      claude_llm:
        base_url: 'https://api.anthropic.com' # Base URL
        llm_api_key: 'YOUR API KEY HERE' # API key
        model: 'claude-3-haiku-20240307' # Model to use

      llama_cpp_llm:
        model_path: '<path-to-gguf-model-file>' # GGUF model file path
        verbose: False # Whether to output verbose information

      ollama_llm:
        base_url: 'http://localhost:11434/v1' # Base URL
        model: 'qwen2.5:latest' # Model to use
        temperature: 1.0 # Temperature, between 0 to 2
        # Time (in seconds) the model stays in memory after inactivity
        # Set to -1 to keep the model in memory forever (even after exiting open llm vtuber)
        keep_alive: -1
        unload_at_exit: True # Unload model from memory on exit

      lmstudio_llm:
        base_url: 'http://localhost:1234/v1'
        model: 'qwen2.5:latest'
        temperature: 1.0 # value between 0 to 2

      openai_llm:
        llm_api_key: 'Your Open AI API key' # OpenAI API key
        model: 'gpt-4o' # Model to use
        temperature: 1.0 # Temperature, between 0 to 2

      gemini_llm:
        llm_api_key: 'Your Gemini API Key' # Gemini API key
        model: 'gemini-2.0-flash-exp' # Model to use
        temperature: 1.0 # Temperature, between 0 to 2

      zhipu_llm:
        llm_api_key: 'Your ZhiPu AI API key' # ZhiPu AI API key
        model: 'glm-4-flash' # Model to use
        temperature: 1.0 # Temperature, between 0 to 2

      deepseek_llm:
        llm_api_key: 'Your DeepSeek API key' # DeepSeek API key
        model: 'deepseek-chat' # Model to use
        temperature: 0.7 # Note: DeepSeek temperature range is 0 to 1

      mistral_llm:
        llm_api_key: 'Your Mistral API key' # Mistral API key
        model: 'pixtral-large-latest' # Model to use
        temperature: 1.0 # Temperature, between 0 to 2

      groq_llm:
        llm_api_key: 'your groq API key' # Groq API key
        model: 'llama-3.3-70b-versatile' # Model to use
        temperature: 1.0 # Temperature, between 0 to 2

  # === Automatic Speech Recognition ===
  asr_config:
    # Speech-to-text model options: 'faster_whisper', 'whisper_cpp', 'whisper', 'azure_asr', 'fun_asr', 'groq_whisper_asr', 'sherpa_onnx_asr'
    asr_model: 'sherpa_onnx_asr' # Speech recognition model to use

    azure_asr:
      api_key: 'azure_api_key' # Azure API key
      region: 'eastus' # Region
      languages: ['en-US', 'zh-CN']  # List of languages to detect

    # Faster Whisper configuration
    faster_whisper:
      model_path: 'large-v3-turbo' # Model path, model name, or hf hub model id
      download_root: 'models/whisper' # Model download root directory
      language: 'zh' # Language: en, zh, or other. Leave empty for auto detection.
      device: 'auto' # Device: cpu, cuda, or auto. faster-whisper does not support mps
      compute_type: 'int8'
      prompt: '' # Prompt to assist in generating correct text

    whisper_cpp:
      # All available models are listed at https://abdeladim-s.github.io/pywhispercpp/#pywhispercpp.constants.AVAILABLE_MODELS
      model_name: 'small' # Model name
      model_dir: 'models/whisper' # Model directory
      print_realtime: False # Whether to print in real-time
      print_progress: False # Whether to print progress
      language: 'auto' # Language: en, zh, auto
      prompt: '' # Prompt to assist in generating correct text

    whisper:
      name: 'medium' # Model name
      download_root: 'models/whisper' # Model download root directory
      device: 'cpu' # Device
      prompt: '' # Prompt to assist in generating correct text

    # FunASR currently requires internet connection at startup to download/check models. You can disconnect after initialization.
    # Alternatively, you can use sherpa onnx asr or Faster-Whisper for a fully offline experience
    fun_asr:
      model_name: 'iic/SenseVoiceSmall' # Or 'paraformer-zh'
      vad_model: 'fsmn-vad' # Only needed when audio length exceeds 30 seconds
      punc_model: 'ct-punc' # Punctuation model
      device: 'cpu' # Device
      disable_update: True # Whether to check for FunASR updates at each startup
      ncpu: 4 # Number of threads for internal CPU operations
      hub: 'ms' # ms (default) downloads models from ModelScope. Use hf to download from Hugging Face.
      use_itn: False # Whether to use number format conversion
      language: 'auto' # zh, en, auto

    # pip install sherpa-onnx
    # Documentation: https://k2-fsa.github.io/sherpa/onnx/index.html
    # ASR model download: https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
    sherpa_onnx_asr:
      model_type: 'sense_voice' # 'transducer', 'paraformer', 'nemo_ctc', 'wenet_ctc', 'whisper', 'tdnn_ctc'
      # Choose one of the following based on model_type:
      # --- For model_type: 'transducer' ---
      # encoder: ''        # Encoder model path (e.g. 'path/to/encoder.onnx')
      # decoder: ''        # Decoder model path (e.g. 'path/to/decoder.onnx')
      # joiner: ''         # Joiner model path (e.g. 'path/to/joiner.onnx')
      # --- For model_type: 'paraformer' ---
      # paraformer: ''     # Paraformer model path (e.g. 'path/to/model.onnx')
      # --- For model_type: 'nemo_ctc' ---
      # nemo_ctc: ''        # NeMo CTC model path (e.g. 'path/to/model.onnx')
      # --- For model_type: 'wenet_ctc' ---
      # wenet_ctc: ''       # WeNet CTC model path (e.g. 'path/to/model.onnx')
      # --- For model_type: 'tdnn_ctc' ---
      # tdnn_model: ''      # TDNN CTC model path (e.g. 'path/to/model.onnx')
      # --- For model_type: 'whisper' ---
      # whisper_encoder: '' # Whisper encoder model path (e.g. 'path/to/encoder.onnx')
      # whisper_decoder: '' # Whisper decoder model path (e.g. 'path/to/decoder.onnx')
      # --- For model_type: 'sense_voice' ---
      # SenseVoice has automatic model download logic implemented, other models need to be downloaded manually
      sense_voice: './models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/model.int8.onnx' # SenseVoice model path (e.g. 'path/to/model.onnx')
      tokens: './models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/tokens.txt' # tokens.txt path (required for all model types)
      # --- Optional parameters (showing default values) ---
      # hotwords_file: ''     # Hotwords file path (if using hotwords)
      # hotwords_score: 1.5   # Hotwords score
      # modeling_unit: ''     # Modeling unit for hotwords (if applicable)
      # bpe_vocab: ''         # BPE vocabulary path (if applicable)
      num_threads: 4 # Number of threads
      # whisper_language: '' # Language for Whisper model (e.g. 'en', 'zh', etc. - if using Whisper)
      # whisper_task: 'transcribe'  # Task for Whisper model ('transcribe' or 'translate' - if using Whisper)
      # whisper_tail_paddings: -1   # Tail paddings for Whisper model (if using Whisper)
      # blank_penalty: 0.0    # Penalty for blank symbol
      # decoding_method: 'greedy_search'  # 'greedy_search' or 'modified_beam_search'
      # debug: False # Enable debug mode
      # sample_rate: 16000 # Sample rate (should match the model's expected sample rate)
      # feature_dim: 80       # Feature dimension (should match the model's expected feature dimension)
      use_itn: True # Enable ITN for SenseVoice model (should be set to False if not using SenseVoice model)
      # Inference platform (cpu or cuda) (cuda requires additional configuration, please refer to documentation)
      provider: 'cpu'

    groq_whisper_asr:
      api_key: ''
      model: 'whisper-large-v3-turbo' # Or 'whisper-large-v3'
      lang: '' # Leave empty for auto

  # =================== Text-to-Speech ===================
  tts_config:
    tts_model: 'edge_tts' # Text-to-speech model to use
    # Text-to-speech model options:
    #   'azure_tts', 'pyttsx3_tts', 'edge_tts', 'bark_tts',
    #   'cosyvoice_tts', 'melo_tts', 'coqui_tts', 'piper_tts',
    #   'fish_api_tts', 'x_tts', 'gpt_sovits_tts', 'sherpa_onnx_tts'
    #   'minimax_tts', 'elevenlabs_tts', 'cartesia_tts'

    siliconflow_tts:
      api_url: "https://api.siliconflow.cn/v1/audio/speech"
      api_key: "your key"  # API key for authentication
      default_model: "FunAudioLLM/CosyVoice2-0.5B"  # Default text-to-speech model
      default_voice: "speech:Dreamflowers:5bdstvc39i:xkqldnpasqmoqbakubom your voice name"  # Default voice configuration, format: "speech:model_name:voice_id:your_voice_name"
      sample_rate: 32000  # Audio sample rate (Hz). Different formats support different sample rates: opus supports 48000Hz; wav/pcm supports 8000, 16000, 24000, 32000, 44100Hz (default 44100Hz); mp3 supports 32000, 44100Hz (default 44100Hz)
      response_format: "mp3"  # Output audio format, supports mp3, opus, wav, pcm
      stream: true
      speed: 1
      gain: 0

    azure_tts:
      api_key: 'azure-api-key' # Azure API key
      region: 'eastus' # Region
      voice: 'en-US-AshleyNeural' # Voice
      pitch: '26' # Pitch adjustment percentage
      rate: '1' # Speech rate

    bark_tts:
      voice: 'v2/en_speaker_1' # Voice

    edge_tts:
      # See documentation: https://github.com/rany2/edge-tts
      # Use `edge-tts --list-voices` to list all available voices
      voice: zh-CN-XiaoxiaoNeural # 'en-US-AvaMultilingualNeural' #'zh-CN-XiaoxiaoNeural' # 'ja-JP-NanamiNeural'

    # pyttsx3_tts has no configuration.

    piper_tts:
      model_path: 'models/piper/zh_CN-huayan-medium.onnx' # Model file path (.onnx file)
      speaker_id: 0             # Speaker ID (used for multi-speaker models, keep 0 for single-speaker models)
      length_scale: 1.0         # Speed control (0.5 = double speed, 1.0 = normal, 2.0 = half speed)
      noise_scale: 0.667        # Audio variation (0.0-1.0, higher means more varied audio, recommended 0.667)
      noise_w: 0.8              # Speaking style variation (0.0-1.0, higher means more diverse speaking style, recommended 0.8)
      volume: 1.0               # Volume (0.0-1.0, 1.0 is normal volume)
      normalize_audio: true     # Whether to normalize audio (recommended to enable for more stable volume)
      use_cuda: false           # Whether to use GPU acceleration (requires onnxruntime-gpu installation)

    cosyvoice_tts: # Cosy Voice TTS connected to gradio webui
      # See their documentation for deployment and meaning of the following configurations
      client_url: 'http://127.0.0.1:50000/' # CosyVoice gradio demo webui url
      mode_checkbox_group: '预训练音色' # Mode checkbox group
      sft_dropdown: '中文女' # Fine-tuned dropdown
      prompt_text: '' # Prompt text
      prompt_wav_upload_url: 'https://github.com/gradio-app/gradio/raw/main/test/test_files/audio_sample.wav' # Prompt wav upload url
      prompt_wav_record_url: 'https://github.com/gradio-app/gradio/raw/main/test/test_files/audio_sample.wav' # Prompt wav record url
      instruct_text: '' # Instruct text
      seed: 0 # Seed
      api_name: '/generate_audio' # API name

    cosyvoice2_tts: # Cosy Voice TTS connected to gradio webui
      # See their documentation for deployment and meaning of the following configurations
      client_url: 'http://127.0.0.1:50000/' # CosyVoice gradio demo webui url
      mode_checkbox_group: '预训练音色' # Mode checkbox group, options: '预训练音色', '3s极速复刻', '跨语种复刻', '自然语言控制'
      sft_dropdown: '中文女' # Fine-tuned dropdown
      prompt_text: '' # Prompt text
      prompt_wav_upload_url: 'https://github.com/gradio-app/gradio/raw/main/test/test_files/audio_sample.wav' # Prompt wav upload url
      prompt_wav_record_url: 'https://github.com/gradio-app/gradio/raw/main/test/test_files/audio_sample.wav' # Prompt wav record url
      instruct_text: '' # Instruct text
      stream: False # Streaming generation
      seed: 0 # Seed
      speed: 1.0 # Speech rate
      api_name: '/generate_audio' # API name

    melo_tts:
      speaker: 'EN-Default' # ZH
      language: 'EN' # ZH
      device: 'auto' # You can manually set it to 'cpu', 'cuda', 'cuda:0', or 'mps'
      speed: 1.0 # Speech rate

    x_tts:
      api_url: 'http://127.0.0.1:8020/tts_to_audio' # API URL
      speaker_wav: 'female' # Speaker WAV file
      language: 'en' # Language

    gpt_sovits_tts:
      # Place the reference audio in the GPT-Sovits root path, or set the path here
      api_url: 'http://127.0.0.1:9880/tts' # API URL
      text_lang: 'zh' # Text language
      ref_audio_path: '' # str.(required) Path to reference audio
      prompt_lang: 'zh' # str.(required) Language of reference audio prompt text
      prompt_text: '' # str.(optional) Prompt text for reference audio
      text_split_method: 'cut5' # Text split method
      batch_size: '1' # Batch size
      media_type: 'wav' # Media type
      streaming_mode: 'false' # Streaming mode

    fish_api_tts:
      # API key for Fish TTS API.
      api_key: ''
      # Reference ID of the voice to use. Get it from [Fish Audio website](https://fish.audio/).
      reference_id: ''
      # 'normal' or 'balanced'. balanced is faster but lower quality.
      latency: 'balanced' # Latency
      base_url: 'https://api.fish.audio' # Base URL

    coqui_tts:
      # Name of the TTS model to use. If empty, the default model will be used
      # Run 'tts --list_models' to list models supported by coqui-tts
      # Some examples:
      # - 'tts_models/en/ljspeech/tacotron2-DDC' (single speaker)
      # - 'tts_models/zh-CN/baker/tacotron2-DDC-GST' (Chinese single speaker)
      # - 'tts_models/multilingual/multi-dataset/your_tts' (multi-speaker)
      # - 'tts_models/multilingual/multi-dataset/xtts_v2' (multi-speaker)
      model_name: 'tts_models/en/ljspeech/tacotron2-DDC' # Model name
      speaker_wav: '' # Speaker WAV file
      language: 'en' # Language
      device: '' # Device

    # pip install sherpa-onnx
    # Documentation: https://k2-fsa.github.io/sherpa/onnx/index.html
    # TTS model download: https://github.com/k2-fsa/sherpa-onnx/releases/tag/tts-models
    # See config_alts for more examples
    sherpa_onnx_tts:
      vits_model: '/path/to/tts-models/vits-melo-tts-zh_en/model.onnx' # VITS model file path
      vits_lexicon: '/path/to/tts-models/vits-melo-tts-zh_en/lexicon.txt' # Lexicon file path (optional)
      vits_tokens: '/path/to/tts-models/vits-melo-tts-zh_en/tokens.txt' # Token file path
      vits_data_dir: '' # '/path/to/tts-models/vits-piper-en_GB-cori-high/espeak-ng-data'  # espeak-ng data path (optional)
      vits_dict_dir: '/path/to/tts-models/vits-melo-tts-zh_en/dict' # Jieba dictionary path (optional, for Chinese)
      tts_rule_fsts: '/path/to/tts-models/vits-melo-tts-zh_en/number.fst,/path/to/tts-models/vits-melo-tts-zh_en/phone.fst,/path/to/tts-models/vits-melo-tts-zh_en/date.fst,/path/to/tts-models/vits-melo-tts-zh_en/new_heteronym.fst' # Rule FST file paths (optional)
      max_num_sentences: 2 # Maximum sentences per batch (or -1 for all)
      sid: 1 # Speaker ID (for multi-speaker models)
      provider: 'cpu' # Use 'cpu', 'cuda' (GPU), or 'coreml' (Apple)
      num_threads: 1 # Number of computation threads
      speed: 1.0 # Speech rate (1.0 is normal)
      debug: false # Enable debug mode (True/False)
    spark_tts:
      api_url: 'http://127.0.0.1:6006/' # Initial API address, using gradio's built-in frontend API. Address: https://github.com/SparkAudio/Spark-TTS
      api_name:  "voice_clone" # Endpoint name, options: voice_clone, voice_creation
      prompt_wav_upload: "https://uploadstatic.mihoyo.com/ys-obc/2022/11/02/16576950/4d9feb71760c5e8eb5f6c700df12fa0c_6824265537002152805.mp3" # Reference audio URL, fill in when api_name = "voice_clone"
      gender:  "female" # Voice gender, fill in when api_name = "voice_creation"
      pitch:  3 # Pitch, fill in when api_name = "voice_creation"
      speed:  3 # Speed, fill in when api_name = "voice_creation"

    openai_tts: # OpenAI compatible TTS (text-to-speech) interface configuration
      # If these settings are provided, they will override the defaults in openai_tts.py
      model: 'kokoro' # Model name expected by the server (e.g. 'tts-1', 'kokoro')
      voice: 'af_sky+af_bella' # Voice name expected by the server (e.g. 'alloy', 'af_sky+af_bella')
      api_key: 'not-needed' # API key if required by the server
      base_url: 'http://localhost:8880/v1' # Base URL of the TTS server
      file_extension: 'mp3' # Audio file format ('mp3' or 'wav')
    # Detailed documentation: https://platform.minimaxi.com/document/Announcement
    minimax_tts:
      group_id: '' # minimax group_id
      api_key: '' # minimax api_key
      # Supported models: 'speech-02-hd', 'speech-02-turbo' (recommended: 'speech-02-turbo')
      model: 'speech-02-turbo' # minimax model name
      voice_id: 'female-shaonv' # minimax voice id, default 'female-shaonv'
      # Custom pronunciation dictionary, default is empty.
      # Example: '{"tone": ["测试/(ce4)(shi4)", "危险/dangerous"]}'
      pronunciation_dict: ''

    elevenlabs_tts:
      api_key: ''
      voice_id: '' # Voice ID from ElevenLabs
      model_id: 'eleven_multilingual_v2' # Model ID (e.g. eleven_multilingual_v2)
      output_format: 'mp3_44100_128' # Output audio format (e.g. mp3_44100_128)
      stability: 0.5 # Voice stability (0.0 to 1.0)
      similarity_boost: 0.5 # Voice similarity boost (0.0 to 1.0)
      style: 0.0 # Voice style exaggeration (0.0 to 1.0)
      use_speaker_boost: true # Enable speaker boost for better quality

    cartesia_tts:
      api_key: ''
      voice_id: '' # Voice ID from Cartesia
      model_id: 'sonic-3' # Model ID (e.g. sonic-3)
      output_format: 'wav' # Output audio format (e.g. wav)
      language: 'en' # Language of output speech (e.g. en)
      emotion: 'neutral' # Emotion guidance
      volume: 1.0 # Voice volume (0.5 to 2.0)
      speed: 1.0 # Voice speed (0.6 to 1.5)

  # =================== Voice Activity Detection ===================
  vad_config:
    vad_model: null

    silero_vad:
      orig_sr: 16000 # Original audio sample rate
      target_sr: 16000 # Target audio sample rate
      prob_threshold: 0.4 # Probability threshold for voice activity detection
      db_threshold: 60 # Decibel threshold for voice activity detection
      required_hits: 3 # Consecutive hits to confirm speech
      required_misses: 24 # Consecutive misses to confirm silence
      smoothing_window: 5 # Smoothing window size for voice activity detection

  tts_preprocessor_config:
    # Settings for text preprocessing before entering TTS

    remove_special_char: True # Remove special characters like emojis from audio generation
    ignore_brackets: True # Remove content enclosed in square brackets from audio generation
    ignore_parentheses: True # Remove content enclosed in parentheses from audio generation
    ignore_asterisks: True # Remove content enclosed in asterisks from audio generation (single and double asterisks)
    ignore_angle_brackets: True # Remove content enclosed in angle brackets from audio generation

    translator_config:
      # For example... you speak and read English subtitles while TTS speaks Japanese, etc.
      translate_audio: False # Warning: Please ensure the translation engine is configured successfully before enabling this option, otherwise translation will fail
      translate_provider: 'deeplx' # Translation provider, currently supports deeplx or tencent

      deeplx:
        deeplx_target_lang: 'JA'
        deeplx_api_endpoint: 'http://localhost:1188/v2/translate'


      # Tencent Text Translation - 5 million characters per month free. Remember to disable post-payment. You need to manually go to Machine Translation Console > System Settings to disable it
      #   https://cloud.tencent.com/document/product/551/35017
      #   https://console.cloud.tencent.com/cam/capi
      tencent:
        secret_id: ''
        secret_key: ''
        region: 'ap-guangzhou'
        source_lang: 'zh'
        target_lang: 'ja'

# Live platform integration
live_config:
  bilibili_live:
    # List of Bilibili live room IDs to monitor (the number in the live room URL)
    room_ids: [1991478060]
    # SESSDATA cookie value (optional, for authenticated requests, allows viewing danmaku sender usernames)
    sessdata: ""
